{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrTorGonz/Homework-2/blob/main/Homework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework 2\n",
        "\n",
        "Authors: Daniel Alejandro Torres González & Juan David Plata Garrido\n"
      ],
      "metadata": {
        "id": "tirbUldjh8Ip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLDbhdY-hN_z",
        "outputId": "7056752b-f8b0-435a-92db-d0535d19f002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma del tensor features: (442, 11)\n",
            "Tipo del tensor features: <dtype: 'float32'>\n",
            "Forma del tensor labels: (442, 1)\n",
            "Tipo del tensor labels: <dtype: 'float32'>\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Leer archivo Excel\n",
        "df = pd.read_excel('Diagnostico.xlsx')\n",
        "\n",
        "# Eliminar filas nulas\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Cambiar los datos de texto a numéricos (columna GENERO)\n",
        "df['GENERO'] = df['GENERO'].map({'MASCULINO': 1, 'FEMENINO': 2})\n",
        "\n",
        "# Separar la última columna (labels) y las demás columnas (features)\n",
        "features_df = df.iloc[:, :-1]  # Todas las columnas excepto la última\n",
        "labels_df = df.iloc[:, -1]     # Última columna\n",
        "\n",
        "# Crear tensores con TensorFlow\n",
        "features_tensor = tf.convert_to_tensor(features_df, dtype=tf.float32)\n",
        "labels_tensor = tf.convert_to_tensor(labels_df.values, dtype=tf.float32)\n",
        "\n",
        "#convertir label a (442,1)\n",
        "labels_tensor = tf.reshape(labels_tensor, (-1, 1))\n",
        "\n",
        "# Información de los tensores\n",
        "print(\"Forma del tensor features:\", features_tensor.shape)\n",
        "print(\"Tipo del tensor features:\", features_tensor.dtype)\n",
        "print(\"Forma del tensor labels:\", labels_tensor.shape)\n",
        "print(\"Tipo del tensor labels:\", labels_tensor.dtype)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribución de datos 80/20**"
      ],
      "metadata": {
        "id": "oJ_kK94jv-sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinar el punto de corte del 80% de los datos\n",
        "split_index = int(0.8 * len(features_tensor))\n",
        "\n",
        "# Dividir features y labels\n",
        "train_features = features_tensor[:split_index]  # 80% inicial\n",
        "test_features = features_tensor[split_index:]   # 20% restante\n",
        "\n",
        "train_labels = labels_tensor[:split_index]  # 80% inicial\n",
        "test_labels = labels_tensor[split_index:]   # 20% restante\n",
        "\n",
        "# Mostrar información de las particiones\n",
        "print(\"Forma de train_features:\", train_features.shape)\n",
        "print(\"Forma de test_features:\", test_features.shape)\n",
        "print(\"Forma de train_labels:\", train_labels.shape)\n",
        "print(\"Forma de test_labels:\", test_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87BfOqgnv9dP",
        "outputId": "846b786c-c329-4744-a694-96bb939de0aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de train_features: (353, 11)\n",
            "Forma de test_features: (89, 11)\n",
            "Forma de train_labels: (353, 1)\n",
            "Forma de test_labels: (89, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leer dataset por lotes**"
      ],
      "metadata": {
        "id": "RfcH8uWxz9gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteraciones para lectura del dataset en formato TensorFlow\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    \"\"\"Construct a TensorFlow data iterator.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(buffer_size=1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 64\n",
        "data_iter = load_array((train_features, train_labels), batch_size)\n",
        "\n",
        "# Prueba de la lectura por minilotes\n",
        "next(iter(data_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1kHwndbz9uw",
        "outputId": "b054a334-f4b3-408a-b8f6-e0a9966c54c3",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 11), dtype=float32, numpy=\n",
              " array([[ 48.    ,   2.    , 174.    ,  99.9   , 123.    ,  97.    ,\n",
              "          44.    , 253.    ,   5.425 , 163.6   ,   6.    ],\n",
              "        [ 59.    ,   2.    , 185.    ,  82.5   ,  96.    ,  85.    ,\n",
              "          54.    , 170.    ,   4.4659,  98.6   ,   3.    ],\n",
              "        [ 19.    ,   1.    , 173.    ,  75.7   ,  83.    ,  84.    ,\n",
              "          46.    , 225.    ,   4.7185, 156.6   ,   5.    ],\n",
              "        [ 58.    ,   2.    , 168.    , 103.6   , 117.    , 109.    ,\n",
              "          44.    , 166.    ,   4.9488,  93.8   ,   4.    ],\n",
              "        [ 38.    ,   1.    , 190.    ,  91.7   ,  84.    ,  87.    ,\n",
              "          42.    , 162.    ,   4.4427, 103.    ,   4.    ],\n",
              "        [ 57.    ,   2.    , 170.    ,  91.3   , 117.    , 113.    ,\n",
              "          40.    , 225.    ,   5.9584, 107.6   ,   6.    ],\n",
              "        [ 57.    ,   2.    , 161.    ,  60.1   , 107.33  , 112.    ,\n",
              "          41.    , 231.    ,   5.0304, 159.4   ,   5.63  ],\n",
              "        [ 59.    ,   2.    , 168.    ,  70.8   ,  90.    ,  91.    ,\n",
              "          46.    , 163.    ,   4.3567, 101.4   ,   4.    ],\n",
              "        [ 49.    ,   2.    , 178.    ,  86.8   ,  89.    ,  97.    ,\n",
              "          37.    , 177.    ,   4.9053, 113.    ,   5.    ],\n",
              "        [ 45.    ,   2.    , 168.    ,  59.8   ,  94.    , 102.    ,\n",
              "          55.    , 169.    ,   4.4543,  96.8   ,   3.    ],\n",
              "        [ 45.    ,   1.    , 186.    ,  83.7   ,  83.    ,  82.    ,\n",
              "          45.    , 177.    ,   4.2195, 118.4   ,   4.    ],\n",
              "        [ 49.    ,   1.    , 190.    ,  73.3   ,  93.    ,  93.    ,\n",
              "          61.    , 184.    ,   4.6052, 103.    ,   3.    ],\n",
              "        [ 43.    ,   1.    , 177.    ,  58.    ,  87.    ,  80.    ,\n",
              "          61.    , 163.    ,   3.7377,  93.6   ,   2.67  ],\n",
              "        [ 48.    ,   1.    , 173.    ,  61.1   ,  98.    ,  78.    ,\n",
              "          46.    , 209.    ,   4.7707, 139.4   ,   5.    ],\n",
              "        [ 34.    ,   2.    , 178.    ,  71.6   ,  75.    , 108.    ,\n",
              "          60.    , 166.    ,   4.2627,  91.8   ,   3.    ],\n",
              "        [ 49.    ,   1.    , 172.    ,  67.2   ,  65.33  ,  60.    ,\n",
              "          62.    , 168.    ,   3.8918,  96.2   ,   2.71  ],\n",
              "        [ 48.    ,   1.    , 165.    ,  55.    ,  95.    ,  85.    ,\n",
              "          53.    , 187.    ,   4.4188, 117.4   ,   4.    ],\n",
              "        [ 48.    ,   1.    , 185.    ,  78.    , 101.    ,  97.    ,\n",
              "          56.    , 110.    ,   4.1271,  41.6   ,   2.    ],\n",
              "        [ 43.    ,   1.    , 190.    , 101.4   , 121.    ,  93.    ,\n",
              "          60.    , 192.    ,   4.0073, 121.    ,   3.    ],\n",
              "        [ 54.    ,   1.    , 173.    ,  69.4   , 110.67  , 108.    ,\n",
              "          48.    , 238.    ,   4.9127, 162.8   ,   4.96  ],\n",
              "        [ 45.    ,   1.    , 181.    ,  66.5   ,  74.33  ,  79.    ,\n",
              "          49.    , 190.    ,   4.3041, 126.2   ,   3.88  ],\n",
              "        [ 36.    ,   1.    , 178.    ,  62.1   ,  71.    ,  92.    ,\n",
              "          97.    , 250.    ,   4.5951, 133.2   ,   3.    ],\n",
              "        [ 51.    ,   2.    , 180.    ,  94.6   , 107.    ,  95.    ,\n",
              "          32.    , 187.    ,   4.382 , 139.    ,   6.    ],\n",
              "        [ 51.    ,   1.    , 180.    ,  81.6   , 103.    ,  90.    ,\n",
              "          37.    , 176.    ,   4.8978, 112.2   ,   5.    ],\n",
              "        [ 28.    ,   2.    , 174.    ,  77.2   ,  99.    ,  94.    ,\n",
              "          46.    , 162.    ,   4.2767, 101.6   ,   4.    ],\n",
              "        [ 29.    ,   1.    , 157.    ,  51.8   ,  71.    ,  90.    ,\n",
              "          38.    , 156.    ,   4.654 ,  97.    ,   4.    ],\n",
              "        [ 47.    ,   1.    , 183.    , 105.8   ,  84.    , 105.    ,\n",
              "          30.    , 154.    ,   5.1985,  88.    ,   5.1   ],\n",
              "        [ 42.    ,   1.    , 150.    ,  71.8   ,  83.    , 101.    ,\n",
              "          53.    , 158.    ,   4.4659,  87.6   ,   3.    ],\n",
              "        [ 57.    ,   2.    , 158.    ,  63.9   ,  96.    , 105.    ,\n",
              "          52.    , 200.    ,   4.3175, 133.    ,   3.85  ],\n",
              "        [ 53.    ,   1.    , 156.    ,  64.5   ,  97.    ,  99.    ,\n",
              "          58.    , 193.    ,   4.1431, 122.4   ,   3.    ],\n",
              "        [ 37.    ,   1.    , 154.    ,  47.9   ,  81.    ,  88.    ,\n",
              "          63.    , 162.    ,   4.0254,  87.8   ,   3.    ],\n",
              "        [ 51.    ,   2.    , 180.    ,  84.9   , 101.    ,  88.    ,\n",
              "          48.    , 161.    ,   4.2047,  99.6   ,   3.    ],\n",
              "        [ 33.    ,   2.    , 152.    ,  58.7   , 102.    , 105.    ,\n",
              "          39.    , 206.    ,   4.8675, 141.    ,   5.    ],\n",
              "        [ 59.    ,   2.    , 182.    ,  90.1   , 107.    ,  93.    ,\n",
              "          39.    , 158.    ,   4.4427, 102.    ,   4.    ],\n",
              "        [ 20.    ,   2.    , 190.    ,  87.4   ,  88.    ,  74.    ,\n",
              "          45.    , 126.    ,   3.7842,  72.2   ,   3.    ],\n",
              "        [ 44.    ,   1.    , 169.    ,  72.5   ,  95.    ,  83.    ,\n",
              "          53.    , 162.    ,   4.4067,  92.6   ,   3.    ],\n",
              "        [ 60.    ,   2.    , 175.    ,  69.8   , 110.    ,  88.    ,\n",
              "          39.    , 245.    ,   4.3944, 189.8   ,   6.    ],\n",
              "        [ 49.    ,   2.    , 152.    ,  73.7   ,  94.    , 122.    ,\n",
              "          34.    , 234.    ,   5.3982, 155.8   ,   7.    ],\n",
              "        [ 53.    ,   1.    , 180.    ,  78.1   , 105.    ,  95.    ,\n",
              "          46.    , 184.    ,   4.8122, 113.4   ,   4.    ],\n",
              "        [ 56.    ,   1.    , 162.    ,  70.9   , 105.    ,  94.    ,\n",
              "          54.    , 247.    ,   5.0876, 160.6   ,   5.    ],\n",
              "        [ 32.    ,   2.    , 171.    ,  64.3   ,  88.    ,  78.    ,\n",
              "          48.    , 137.    ,   3.9512,  78.6   ,   3.    ],\n",
              "        [ 41.    ,   2.    , 183.    , 107.2   , 109.    , 103.    ,\n",
              "          49.    , 251.    ,   5.0562, 170.6   ,   5.    ],\n",
              "        [ 58.    ,   2.    , 180.    ,  94.    ,  85.    ,  86.    ,\n",
              "          36.    , 156.    ,   3.989 , 109.2   ,   4.    ],\n",
              "        [ 31.    ,   1.    , 168.    ,  65.8   ,  85.    ,  77.    ,\n",
              "          43.    , 190.    ,   4.3944, 130.8   ,   4.    ],\n",
              "        [ 51.    ,   1.    , 161.    ,  63.5   ,  79.    ,  91.    ,\n",
              "          65.    , 212.    ,   4.5218, 128.6   ,   3.    ],\n",
              "        [ 51.    ,   2.    , 169.    ,  74.    ,  76.    ,  96.    ,\n",
              "          39.    , 240.    ,   5.0752, 169.    ,   6.    ],\n",
              "        [ 44.    ,   1.    , 178.    ,  73.2   ,  87.    ,  72.    ,\n",
              "          77.    , 213.    ,   3.8712, 126.4   ,   3.    ],\n",
              "        [ 36.    ,   2.    , 159.    ,  60.9   , 112.    ,  95.    ,\n",
              "          35.    , 193.    ,   5.1059, 125.    ,   6.    ],\n",
              "        [ 34.    ,   2.    , 160.    ,  52.7   ,  98.    ,  92.    ,\n",
              "          83.    , 183.    ,   3.6889,  92.    ,   2.    ],\n",
              "        [ 37.    ,   2.    , 154.    ,  65.7   ,  93.    ,  88.    ,\n",
              "          30.    , 180.    ,   5.0304, 119.4   ,   6.    ],\n",
              "        [ 41.    ,   1.    , 182.    ,  68.9   ,  86.    ,  89.    ,\n",
              "          83.    , 223.    ,   4.0775, 128.2   ,   3.    ],\n",
              "        [ 41.    ,   1.    , 154.    ,  47.9   ,  62.    ,  89.    ,\n",
              "          50.    , 153.    ,   4.2485,  89.    ,   3.    ],\n",
              "        [ 40.    ,   1.    , 153.    ,  71.9   ,  99.    ,  85.    ,\n",
              "          50.    , 177.    ,   5.3375,  85.4   ,   4.    ],\n",
              "        [ 50.    ,   1.    , 190.    , 111.9   , 123.    ,  88.    ,\n",
              "          48.    , 178.    ,   4.8283, 105.    ,   4.    ],\n",
              "        [ 60.    ,   1.    , 165.    ,  55.5   , 105.    ,  79.    ,\n",
              "          99.    , 198.    ,   4.6347,  78.4   ,   2.    ],\n",
              "        [ 40.    ,   2.    , 159.    ,  80.6   ,  95.    ,  93.    ,\n",
              "          38.    , 198.    ,   4.804 , 135.6   ,   5.    ],\n",
              "        [ 54.    ,   1.    , 158.    ,  56.4   ,  90.    ,  92.    ,\n",
              "          64.    , 183.    ,   4.3041, 104.2   ,   3.    ],\n",
              "        [ 51.    ,   1.    , 157.    ,  77.6   ,  93.    , 117.    ,\n",
              "          49.    , 231.    ,   5.2523, 144.    ,   4.7   ],\n",
              "        [ 58.    ,   2.    , 171.    , 111.1   , 103.    ,  98.    ,\n",
              "          22.    , 150.    ,   4.6444, 107.2   ,   7.    ],\n",
              "        [ 53.    ,   1.    , 171.    ,  64.6   ,  93.    ,  96.    ,\n",
              "          46.    , 134.    ,   4.0775,  76.2   ,   3.    ],\n",
              "        [ 36.    ,   1.    , 153.    ,  70.2   ,  95.    ,  85.    ,\n",
              "          42.    , 201.    ,   5.1299, 125.2   ,   4.79  ],\n",
              "        [ 35.    ,   2.    , 177.    ,  75.5   ,  94.67  ,  94.    ,\n",
              "          32.    , 155.    ,   4.852 ,  97.4   ,   4.84  ],\n",
              "        [ 56.    ,   1.    , 171.    ,  75.1   ,  80.    ,  95.    ,\n",
              "          59.    , 244.    ,   5.118 , 151.6   ,   4.    ],\n",
              "        [ 37.    ,   1.    , 170.    ,  67.3   ,  88.    ,  82.    ,\n",
              "          65.    , 223.    ,   4.3567, 142.    ,   3.4   ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n",
              " array([[2.52],\n",
              "        [2.  ],\n",
              "        [2.  ],\n",
              "        [3.36],\n",
              "        [0.97],\n",
              "        [3.1 ],\n",
              "        [1.2 ],\n",
              "        [0.83],\n",
              "        [1.11],\n",
              "        [1.35],\n",
              "        [0.64],\n",
              "        [1.53],\n",
              "        [0.9 ],\n",
              "        [0.83],\n",
              "        [0.42],\n",
              "        [0.65],\n",
              "        [0.79],\n",
              "        [0.4 ],\n",
              "        [1.13],\n",
              "        [1.9 ],\n",
              "        [0.96],\n",
              "        [0.57],\n",
              "        [2.44],\n",
              "        [2.92],\n",
              "        [0.6 ],\n",
              "        [1.62],\n",
              "        [2.72],\n",
              "        [0.85],\n",
              "        [1.77],\n",
              "        [0.49],\n",
              "        [0.59],\n",
              "        [0.44],\n",
              "        [1.79],\n",
              "        [1.27],\n",
              "        [0.71],\n",
              "        [0.25],\n",
              "        [2.53],\n",
              "        [2.68],\n",
              "        [0.66],\n",
              "        [0.67],\n",
              "        [0.72],\n",
              "        [1.86],\n",
              "        [1.45],\n",
              "        [2.14],\n",
              "        [0.97],\n",
              "        [1.16],\n",
              "        [0.52],\n",
              "        [1.63],\n",
              "        [0.45],\n",
              "        [1.42],\n",
              "        [0.72],\n",
              "        [0.77],\n",
              "        [0.99],\n",
              "        [2.8 ],\n",
              "        [1.7 ],\n",
              "        [1.78],\n",
              "        [0.72],\n",
              "        [1.73],\n",
              "        [3.41],\n",
              "        [1.78],\n",
              "        [2.2 ],\n",
              "        [0.58],\n",
              "        [0.95],\n",
              "        [0.94]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**inicializar parametros y modelo**"
      ],
      "metadata": {
        "id": "bqtcVqal7HQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parametros\n",
        "initializer = tf.keras.initializers.LecunNormal()\n",
        "\n",
        "\n",
        "#modelo\n",
        "modelo1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, kernel_initializer=initializer)\n",
        "])\n",
        "\n",
        "#perdida\n",
        "# Pérdida por error cuadrático medio\n",
        "loss = tf.keras.losses.Huber(delta=1.0)\n",
        "\n",
        "#algoritmo de optimización\n",
        "trainer = tf.keras.optimizers.Adam(learning_rate=0.005)\n"
      ],
      "metadata": {
        "id": "h0EVE1bp7G_G"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento**"
      ],
      "metadata": {
        "id": "5BOBJFUhC8x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter:\n",
        "        with tf.GradientTape() as tape:\n",
        "            l = loss(modelo1(X, training=True), y)\n",
        "        grads = tape.gradient(l, modelo1.trainable_variables)\n",
        "        trainer.apply_gradients(zip(grads, modelo1.trainable_variables))\n",
        "    l = loss(modelo1(train_features), train_labels)\n",
        "    print(f'época {epoch + 1}, pérdida {l:f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "--owangMC82q",
        "outputId": "cf7aa050-8f23-40df-f517-eda483d3d619"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "época 1, pérdida 172.226013\n",
            "época 2, pérdida 147.191727\n",
            "época 3, pérdida 122.176643\n",
            "época 4, pérdida 97.148827\n",
            "época 5, pérdida 72.112831\n",
            "época 6, pérdida 47.095871\n",
            "época 7, pérdida 22.223131\n",
            "época 8, pérdida 6.196764\n",
            "época 9, pérdida 13.135540\n",
            "época 10, pérdida 9.889647\n",
            "época 11, pérdida 6.360098\n",
            "época 12, pérdida 7.853591\n",
            "época 13, pérdida 6.260232\n",
            "época 14, pérdida 6.503463\n",
            "época 15, pérdida 6.103397\n",
            "época 16, pérdida 6.087951\n",
            "época 17, pérdida 5.903990\n",
            "época 18, pérdida 5.891692\n",
            "época 19, pérdida 5.806950\n",
            "época 20, pérdida 5.762918\n",
            "época 21, pérdida 5.715695\n",
            "época 22, pérdida 5.669023\n",
            "época 23, pérdida 5.621439\n",
            "época 24, pérdida 5.575863\n",
            "época 25, pérdida 5.523025\n",
            "época 26, pérdida 5.471646\n",
            "época 27, pérdida 5.426051\n",
            "época 28, pérdida 5.391691\n",
            "época 29, pérdida 5.318589\n",
            "época 30, pérdida 5.268072\n",
            "época 31, pérdida 5.214972\n",
            "época 32, pérdida 5.162135\n",
            "época 33, pérdida 5.134873\n",
            "época 34, pérdida 5.062383\n",
            "época 35, pérdida 5.006162\n",
            "época 36, pérdida 4.951899\n",
            "época 37, pérdida 4.890641\n",
            "época 38, pérdida 4.831234\n",
            "época 39, pérdida 4.789714\n",
            "época 40, pérdida 4.716520\n",
            "época 41, pérdida 4.658803\n",
            "época 42, pérdida 4.602030\n",
            "época 43, pérdida 4.547105\n",
            "época 44, pérdida 4.485808\n",
            "época 45, pérdida 4.422678\n",
            "época 46, pérdida 4.366621\n",
            "época 47, pérdida 4.300385\n",
            "época 48, pérdida 4.242959\n",
            "época 49, pérdida 4.180183\n",
            "época 50, pérdida 4.122746\n",
            "época 51, pérdida 4.062482\n",
            "época 52, pérdida 4.005105\n",
            "época 53, pérdida 3.944580\n",
            "época 54, pérdida 3.880395\n",
            "época 55, pérdida 3.809063\n",
            "época 56, pérdida 3.746402\n",
            "época 57, pérdida 3.686743\n",
            "época 58, pérdida 3.645414\n",
            "época 59, pérdida 3.565758\n",
            "época 60, pérdida 3.507415\n",
            "época 61, pérdida 3.437025\n",
            "época 62, pérdida 3.373343\n",
            "época 63, pérdida 3.312591\n",
            "época 64, pérdida 3.252387\n",
            "época 65, pérdida 3.194781\n",
            "época 66, pérdida 3.131582\n",
            "época 67, pérdida 3.098403\n",
            "época 68, pérdida 3.005655\n",
            "época 69, pérdida 2.945145\n",
            "época 70, pérdida 2.884948\n",
            "época 71, pérdida 2.828523\n",
            "época 72, pérdida 2.763072\n",
            "época 73, pérdida 2.704048\n",
            "época 74, pérdida 2.680742\n",
            "época 75, pérdida 2.642100\n",
            "época 76, pérdida 2.642526\n",
            "época 77, pérdida 2.507722\n",
            "época 78, pérdida 2.458322\n",
            "época 79, pérdida 2.395005\n",
            "época 80, pérdida 2.316103\n",
            "época 81, pérdida 2.245951\n",
            "época 82, pérdida 2.182907\n",
            "época 83, pérdida 2.127069\n",
            "época 84, pérdida 2.086185\n",
            "época 85, pérdida 2.046615\n",
            "época 86, pérdida 1.955715\n",
            "época 87, pérdida 1.898481\n",
            "época 88, pérdida 1.846706\n",
            "época 89, pérdida 1.787024\n",
            "época 90, pérdida 1.731299\n",
            "época 91, pérdida 1.696488\n",
            "época 92, pérdida 1.673523\n",
            "época 93, pérdida 1.572527\n",
            "época 94, pérdida 1.515326\n",
            "época 95, pérdida 1.515040\n",
            "época 96, pérdida 1.411658\n",
            "época 97, pérdida 1.362917\n",
            "época 98, pérdida 1.343566\n",
            "época 99, pérdida 1.267932\n",
            "época 100, pérdida 1.281578\n",
            "época 101, pérdida 1.250891\n",
            "época 102, pérdida 1.200185\n",
            "época 103, pérdida 1.144928\n",
            "época 104, pérdida 1.243943\n",
            "época 105, pérdida 1.129080\n",
            "época 106, pérdida 1.005839\n",
            "época 107, pérdida 1.122703\n",
            "época 108, pérdida 0.955985\n",
            "época 109, pérdida 0.896906\n",
            "época 110, pérdida 1.009268\n",
            "época 111, pérdida 0.796690\n",
            "época 112, pérdida 1.028415\n",
            "época 113, pérdida 0.792087\n",
            "época 114, pérdida 0.737835\n",
            "época 115, pérdida 0.752216\n",
            "época 116, pérdida 0.697190\n",
            "época 117, pérdida 0.676813\n",
            "época 118, pérdida 0.611675\n",
            "época 119, pérdida 0.620618\n",
            "época 120, pérdida 0.586073\n",
            "época 121, pérdida 0.543537\n",
            "época 122, pérdida 0.558141\n",
            "época 123, pérdida 0.516911\n",
            "época 124, pérdida 0.492209\n",
            "época 125, pérdida 0.524587\n",
            "época 126, pérdida 0.473917\n",
            "época 127, pérdida 0.470836\n",
            "época 128, pérdida 0.475217\n",
            "época 129, pérdida 0.428967\n",
            "época 130, pérdida 0.429692\n",
            "época 131, pérdida 0.411316\n",
            "época 132, pérdida 0.407149\n",
            "época 133, pérdida 0.395820\n",
            "época 134, pérdida 0.389067\n",
            "época 135, pérdida 0.406539\n",
            "época 136, pérdida 0.384770\n",
            "época 137, pérdida 0.381580\n",
            "época 138, pérdida 0.374967\n",
            "época 139, pérdida 0.395113\n",
            "época 140, pérdida 0.425571\n",
            "época 141, pérdida 0.352076\n",
            "época 142, pérdida 0.409501\n",
            "época 143, pérdida 0.368518\n",
            "época 144, pérdida 0.414018\n",
            "época 145, pérdida 0.376823\n",
            "época 146, pérdida 0.370982\n",
            "época 147, pérdida 0.332719\n",
            "época 148, pérdida 0.383629\n",
            "época 149, pérdida 0.347319\n",
            "época 150, pérdida 0.364149\n",
            "época 151, pérdida 0.323980\n",
            "época 152, pérdida 0.315328\n",
            "época 153, pérdida 0.312490\n",
            "época 154, pérdida 0.330109\n",
            "época 155, pérdida 0.313174\n",
            "época 156, pérdida 0.328070\n",
            "época 157, pérdida 0.302537\n",
            "época 158, pérdida 0.384831\n",
            "época 159, pérdida 0.440644\n",
            "época 160, pérdida 0.344437\n",
            "época 161, pérdida 0.355303\n",
            "época 162, pérdida 0.296919\n",
            "época 163, pérdida 0.290995\n",
            "época 164, pérdida 0.287944\n",
            "época 165, pérdida 0.301855\n",
            "época 166, pérdida 0.284695\n",
            "época 167, pérdida 0.283230\n",
            "época 168, pérdida 0.278904\n",
            "época 169, pérdida 0.280545\n",
            "época 170, pérdida 0.275476\n",
            "época 171, pérdida 0.406786\n",
            "época 172, pérdida 0.582339\n",
            "época 173, pérdida 0.511658\n",
            "época 174, pérdida 0.449455\n",
            "época 175, pérdida 0.268952\n",
            "época 176, pérdida 0.267559\n",
            "época 177, pérdida 0.265999\n",
            "época 178, pérdida 0.264174\n",
            "época 179, pérdida 0.266533\n",
            "época 180, pérdida 0.283951\n",
            "época 181, pérdida 0.275466\n",
            "época 182, pérdida 0.267996\n",
            "época 183, pérdida 0.258482\n",
            "época 184, pérdida 0.257620\n",
            "época 185, pérdida 0.256096\n",
            "época 186, pérdida 0.295620\n",
            "época 187, pérdida 0.252110\n",
            "época 188, pérdida 0.371522\n",
            "época 189, pérdida 0.397009\n",
            "época 190, pérdida 0.254593\n",
            "época 191, pérdida 0.256045\n",
            "época 192, pérdida 0.284117\n",
            "época 193, pérdida 0.257792\n",
            "época 194, pérdida 0.244327\n",
            "época 195, pérdida 0.247020\n",
            "época 196, pérdida 0.243473\n",
            "época 197, pérdida 0.243703\n",
            "época 198, pérdida 0.238172\n",
            "época 199, pérdida 0.311514\n",
            "época 200, pérdida 0.328208\n",
            "época 201, pérdida 0.261550\n",
            "época 202, pérdida 0.255802\n",
            "época 203, pérdida 0.233364\n",
            "época 204, pérdida 0.232847\n",
            "época 205, pérdida 0.328293\n",
            "época 206, pérdida 0.465369\n",
            "época 207, pérdida 0.241324\n",
            "época 208, pérdida 0.334287\n",
            "época 209, pérdida 0.261416\n",
            "época 210, pérdida 0.249505\n",
            "época 211, pérdida 0.237885\n",
            "época 212, pérdida 0.226802\n",
            "época 213, pérdida 0.279292\n",
            "época 214, pérdida 0.224014\n",
            "época 215, pérdida 0.252970\n",
            "época 216, pérdida 0.313534\n",
            "época 217, pérdida 0.220515\n",
            "época 218, pérdida 0.219956\n",
            "época 219, pérdida 0.231747\n",
            "época 220, pérdida 0.291740\n",
            "época 221, pérdida 0.230102\n",
            "época 222, pérdida 0.221119\n",
            "época 223, pérdida 0.281844\n",
            "época 224, pérdida 0.223295\n",
            "época 225, pérdida 0.364504\n",
            "época 226, pérdida 0.224671\n",
            "época 227, pérdida 0.265586\n",
            "época 228, pérdida 0.219297\n",
            "época 229, pérdida 0.353597\n",
            "época 230, pérdida 0.375077\n",
            "época 231, pérdida 0.238710\n",
            "época 232, pérdida 0.233782\n",
            "época 233, pérdida 0.282522\n",
            "época 234, pérdida 0.208903\n",
            "época 235, pérdida 0.239730\n",
            "época 236, pérdida 0.257399\n",
            "época 237, pérdida 0.215144\n",
            "época 238, pérdida 0.219833\n",
            "época 239, pérdida 0.270355\n",
            "época 240, pérdida 0.211663\n",
            "época 241, pérdida 0.218110\n",
            "época 242, pérdida 0.311948\n",
            "época 243, pérdida 0.210253\n",
            "época 244, pérdida 0.215943\n",
            "época 245, pérdida 0.242602\n",
            "época 246, pérdida 0.275698\n",
            "época 247, pérdida 0.279272\n",
            "época 248, pérdida 0.253940\n",
            "época 249, pérdida 0.240752\n",
            "época 250, pérdida 0.364758\n",
            "época 251, pérdida 0.209125\n",
            "época 252, pérdida 0.245783\n",
            "época 253, pérdida 0.199359\n",
            "época 254, pérdida 0.255826\n",
            "época 255, pérdida 0.216472\n",
            "época 256, pérdida 0.203264\n",
            "época 257, pérdida 0.228782\n",
            "época 258, pérdida 0.195764\n",
            "época 259, pérdida 0.199561\n",
            "época 260, pérdida 0.198140\n",
            "época 261, pérdida 0.196042\n",
            "época 262, pérdida 0.202684\n",
            "época 263, pérdida 0.272948\n",
            "época 264, pérdida 0.200587\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-785fa099757c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'época {epoch + 1}, pérdida {l:f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Apply gradient updates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m_backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;31m# Run update step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             self._backend_update_step(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m_backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_reduce_sum_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_tf_update_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m_distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             distribution.extended.update(\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3003\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   3004\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3006\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m       return self._replica_ctx_update(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4073\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4074\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4075\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4077\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4079\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4080\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4081\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4082\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4083\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    129\u001b[0m     ):\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mlocal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         beta_1_power = ops.power(\n\u001b[1;32m    121\u001b[0m             \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         )\n\u001b[1;32m   1020\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   2111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2114\u001b[0m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[1;32m   2115\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTzycVttap-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}